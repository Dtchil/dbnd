import logging
import os
import sys
import time

from typing import List, Union

from airflow import DAG
from airflow.bin.cli import set_is_paused
from airflow.models import BaseOperator

from dbnd import new_dbnd_context, override, task
from dbnd._core.configuration.dbnd_config import config
from dbnd._core.configuration.environ_config import DBND_RESUBMIT_RUN, DBND_RUN_UID
from dbnd._core.configuration.scheduler_file_config_loader import (
    SchedulerFileConfigLoader,
)
from dbnd._core.constants import TaskExecutorType
from dbnd._core.run.databand_run import DatabandRun
from dbnd._core.settings.engine import LocalMachineEngineConfig
from dbnd._core.tracking.tracking_info_run import ScheduledRunInfo
from dbnd._core.utils.string_utils import clean_job_name
from dbnd._core.utils.timezone import convert_to_utc
from dbnd.api import scheduler_api_client
from dbnd.tasks.basics.shell import bash_cmd


logger = logging.getLogger(__name__)


class DbndSchedulerDBDagsProvider(object):
    # by default only run the file sync if we are in the scheduler (and not the webserver)
    def __init__(self):
        self.last_refresh = 0
        self.scheduled_jobs = []
        if (
            config.get("scheduler", "always_file_sync") or ("scheduler" in sys.argv)
        ) and not config.get("scheduler", "never_file_sync"):
            self.file_config_loader = SchedulerFileConfigLoader()
            logger.info("scheduler file syncing active")
        else:
            self.file_config_loader = None
            logger.info("scheduler file syncing disabled")

        self.refresh_interval = config.get("scheduler", "refresh_interval")
        self.default_retries = config.get("scheduler", "default_retries")

    def get_dags(self):  # type: () -> List[DAG]
        logger.debug("about to get scheduler job dags from dbnd db")
        self.refresh_scheduled_jobs(always_refresh=True)
        dags = []
        for job in self.scheduled_jobs:
            dag = self.job_to_dag(job)
            dag.sync_to_db()
            set_is_paused(
                not job["active"]
                or (
                    "validation_errors" in job and job["validation_errors"] is not None
                ),
                None,
                dag,
            )
            dags.append(dag)
        return dags

    def refresh_scheduled_jobs(self, always_refresh=False):
        now = time.time()
        if always_refresh or now - self.refresh_interval > self.last_refresh:
            self.last_refresh = now
        else:
            return

        changes = self.file_config_loader.sync() if self.file_config_loader else None
        if changes:
            logger.info(
                "[databand scheduler] changes in scheduler config file synced: %s"
                % ", ".join(("%s: %s" % (key, changes[key]) for key in changes))
            )

        self.scheduled_jobs = self.get_scheduled_jobs()

    def get_scheduled_jobs(self):  # type: () -> List[dict]
        return [
            s
            for s in scheduler_api_client.get_scheduled_jobs()
            if "validation_errors" not in s or not s["validation_errors"]
        ]

    def job_to_dag(self, job):  # type: (dict) -> Union[DAG, None]
        start_day = convert_to_utc(job["start_date"])
        end_date = convert_to_utc(job["end_date"])

        default_args = {
            "owner": job["create_user"] or "",
            "depends_on_past": job["depends_on_past"],
            "start_date": job["start_date"],
            # "email": ["airflow@example.com"],
            # "email_on_failure": False,
            # "email_on_retry": False,
            # "retry_delay": timedelta(minutes=5),
            "end_date": end_date,
        }

        job_name = clean_job_name(job["name"])
        dag = DAG(
            "%s__launcher" % job_name,
            start_date=start_day,
            default_args=default_args,
            schedule_interval=job["schedule_interval"],
            catchup=job["catchup"],
        )
        # when running in dbnd scheduler mode (not drop-in into a vanilla Airflow instance)
        # this DAG is actually a DatabandDAG
        # that assumes this dag was generated by dbnd so it would create a run in the db,
        # but it will be empty and useless
        # dag.dbnd_tracking = False

        DbndSchedulerOperator(
            task_id="launcher",
            scheduled_cmd=job["cmd"],
            dag=dag,
            scheduled_job_name=job_name,
            scheduled_job_uid=job["uid"],
            retries=job["retries"] or self.default_retries,
        )

        return dag


class DbndSchedulerOperator(BaseOperator):
    template_fields = ("scheduled_cmd",)
    template_ext = (".sh", ".bash")
    ui_color = "#f0ede4"

    def __init__(self, scheduled_cmd, scheduled_job_name, scheduled_job_uid, **kwargs):
        super(DbndSchedulerOperator, self).__init__(**kwargs)
        self.scheduled_job_name = scheduled_job_name
        self.scheduled_job_uid = scheduled_job_uid
        self.scheduled_cmd = scheduled_cmd

    def execute(self, context):
        scheduled_run_info = ScheduledRunInfo(
            scheduled_job_uid=self.scheduled_job_uid,
            scheduled_job_dag_run_id=context.get("dag_run").id,
            scheduled_date=context.get("task_instance").execution_date,
        )

        launcher_task = launcher.task(
            scheduled_cmd=self.scheduled_cmd,
            task_name=context.get("dag").dag_id,
            task_version="now",
        )

        with new_dbnd_context(
            name="airflow",
            conf={
                LocalMachineEngineConfig.task_executor_type: override(
                    TaskExecutorType.local
                ),
                LocalMachineEngineConfig.parallel: override(False),
            },
        ) as dc:
            dc.dbnd_run_task(
                task_or_task_name=launcher_task, scheduled_run_info=scheduled_run_info
            )


@task
def launcher(scheduled_cmd):
    env = os.environ.copy()
    env[DBND_RUN_UID] = str(DatabandRun.get_instance().run_uid)
    env[DBND_RESUBMIT_RUN] = "true"
    return bash_cmd.func(cmd=scheduled_cmd, env=env, dbnd_env=False)


def get_dags():
    from dbnd._core.errors.base import DatabandConnectionException, DatabandApiError

    try:
        # let be sure that we are loaded
        config.load_system_configs()
        dags = DbndSchedulerDBDagsProvider().get_dags()

        logger.info("providing %s dags from scheduled jobs" % len(dags))
        return dags
    except (DatabandConnectionException, DatabandApiError) as e:
        logger.error(str(e))
    except Exception as e:
        raise e
